{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62e2d75a",
   "metadata": {},
   "source": [
    "### How It Works:\n",
    "#### YOLO Loading:\n",
    "The **yolov3.weights and yolov3.cfg files** are loaded using OpenCVâ€™s cv2.dnn.readNet() function. The coco.names file is used to map class IDs to class names.\n",
    "\n",
    "1. Blob Creation:\n",
    "\n",
    "The frame from the video is converted into a blob format (a format that YOLO requires for input). It normalizes the pixel values and resizes the frame to 416x416 for input into the neural network.\n",
    "\n",
    "2. YOLO Forward Pass:\n",
    "\n",
    "The forward pass is run on the YOLO model, generating predictions for each detected object in the frame.\n",
    "\n",
    "3. Filtering Predictions:\n",
    "\n",
    "Only predictions with a confidence level higher than 0.5 are considered, and only the \"person\" class (class_id = 0) is used for face detection.\n",
    "\n",
    "4. Non-Maxima Suppression (NMS):\n",
    "\n",
    "This is used to remove redundant boxes that overlap too much with lower confidence.\n",
    "\n",
    "5. Bounding Boxes:\n",
    "\n",
    "Bounding boxes are drawn on the detected \"person\" in the video feed, displaying the label (in this case \"person\") and the confidence score.\n",
    "\n",
    "## Requirements:\n",
    "1. OpenCV (pip install opencv-python)\n",
    "2. OpenCV DNN module support\n",
    "3. YOLOv3 weights, configuration files, and COCO data (as mentioned earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be641bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")\n",
    "layer_names = net.getLayerNames()\n",
    "\n",
    "# The corrected way to extract the output layers\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63b15f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COCO class labels\n",
    "with open(\"coco.data\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Load the video from webcam (use 'video.mp4' for video file)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture video\")\n",
    "        break\n",
    "\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Prepare the image for YOLO\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Information to show on the screen\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    # Loop over each detection from YOLO\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]  # Skip the first 5 values (box coordinates + confidence)\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            # Filter out weak detections by ensuring the confidence is above a threshold\n",
    "            if confidence > 0.5 and class_id == 0:  # Class '0' is 'person' in COCO dataset\n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maxima suppression to eliminate redundant overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Draw the resulting bounding boxes\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = confidences[i]\n",
    "            color = (0, 255, 0)  # Green bounding box\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} {confidence:.2f}\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('YOLO Face Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close display windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d250e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
